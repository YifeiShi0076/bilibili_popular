{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb5ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from lxml import html\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import urllib.parse\n",
    "import json\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52de65b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871cac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"accept-encoding\":\"gzip, deflate, br, zstd\",\n",
    "    \"accept-language\":\"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "    \"cache-control\":\"max-age=0\",\n",
    "    \"cookie\": os.getenv(\"BILIBILI_COOKIE\"),\n",
    "    \"priority\":\"u=0, i\",\n",
    "    \"sec-ch-ua\":'''\"Chromium\";v=\"136\", \"Google Chrome\";v=\"136\", \"Not.A/Brand\";v=\"99\"''',\n",
    "    \"sec-ch-ua-mobile\":\"?0\",\n",
    "    \"sec-ch-ua-platform\":'''\"Windows\"''',\n",
    "    \"sec-fetch-dest\":\"document\",\n",
    "    \"sec-fetch-mode\":\"navigate\",\n",
    "    \"sec-fetch-site\":\"none\",\n",
    "    \"sec-fetch-user\":\"?1\",\n",
    "    \"upgrade-insecure-requests\":\"1\",\n",
    "    \"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdd9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxyPool:\n",
    "    def __init__(self, auth_key=os.getenv('AUTH_KEY'), \n",
    "                 password=os.getenv('AUTH_PASSWORD'), \n",
    "                 max_proxies=20, \n",
    "                 min_proxies=5):\n",
    "        self.auth_key = auth_key\n",
    "        self.password = password\n",
    "        self.max_proxies = max_proxies\n",
    "        self.min_proxies = min_proxies\n",
    "        self.proxy_pool = []\n",
    "        self.lock = False\n",
    "        self.test_url = \"https://test.ipw.cn\"\n",
    "        \n",
    "    def fetch_proxies(self, num=5):\n",
    "        if self.lock:\n",
    "            return []\n",
    "            \n",
    "        self.lock = True\n",
    "        try:\n",
    "            url = f\"https://share.proxy.qg.net/get?key={self.auth_key}&num={num}\"\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                proxy_data = response.json()\n",
    "                if proxy_data.get('data'):\n",
    "                    new_proxies = [{\n",
    "                        'server': proxy['server'],\n",
    "                        'proxy_url': f\"http://{self.auth_key}:{self.password}@{proxy['server']}\"\n",
    "                    } for proxy in proxy_data['data']]\n",
    "                    return new_proxies\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"获取代理失败: {e}\")\n",
    "            return []\n",
    "        finally:\n",
    "            self.lock = False\n",
    "    \n",
    "    def add_proxies(self, proxies):\n",
    "        existing_servers = {p['server'] for p in self.proxy_pool}\n",
    "        for proxy in proxies:\n",
    "            if proxy['server'] not in existing_servers:\n",
    "                self.proxy_pool.append(proxy)\n",
    "                existing_servers.add(proxy['server'])\n",
    "                \n",
    "        while len(self.proxy_pool) > self.max_proxies:\n",
    "            self.proxy_pool.pop(random.randint(0, len(self.proxy_pool)-1))\n",
    "    \n",
    "    def check_and_refill(self):\n",
    "        if len(self.proxy_pool) < self.min_proxies:\n",
    "            needed = self.max_proxies - len(self.proxy_pool)\n",
    "            new_proxies = self.fetch_proxies(min(needed, 10))\n",
    "            if new_proxies:\n",
    "                self.add_proxies(new_proxies)\n",
    "                print(f\"已补充 {len(new_proxies)} 个新代理\")\n",
    "    \n",
    "    def get_proxy(self):\n",
    "        self.check_and_refill()\n",
    "        if not self.proxy_pool:\n",
    "            return None\n",
    "        return random.choice(self.proxy_pool)\n",
    "    \n",
    "    def remove_proxy(self, proxy):\n",
    "        if proxy in self.proxy_pool:\n",
    "            self.proxy_pool.remove(proxy)\n",
    "    \n",
    "    def validate_proxy(self, proxy, timeout=5):\n",
    "        try:\n",
    "            proxies = {\n",
    "                \"http\": proxy['proxy_url'],\n",
    "                \"https\": proxy['proxy_url']\n",
    "            }\n",
    "            response = requests.get(self.test_url, proxies=proxies, timeout=timeout)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def auto_maintain(self, interval=300):\n",
    "        while True:\n",
    "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                results = list(executor.map(self.validate_proxy, self.proxy_pool))\n",
    "            \n",
    "            valid_proxies = [p for p, valid in zip(self.proxy_pool, results) if valid]\n",
    "            removed = len(self.proxy_pool) - len(valid_proxies)\n",
    "            if removed > 0:\n",
    "                print(f\"移除了 {removed} 个无效代理\")\n",
    "            self.proxy_pool = valid_proxies\n",
    "            \n",
    "            self.check_and_refill()\n",
    "            time.sleep(interval)\n",
    "\n",
    "    def get_proxies_for_requests(self, proxy):\n",
    "        if not proxy:\n",
    "            return None\n",
    "        return {\n",
    "            \"http\": proxy['proxy_url'],\n",
    "            \"https\": proxy['proxy_url']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c31f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilibiliCrawler:\n",
    "\n",
    "    COLUMNS = [\n",
    "        'date_name', 'aid', 'tid', 'tname', 'title', 'duration',\n",
    "        'owner_mid', 'owner_name', 'view', 'danmaku', 'reply',\n",
    "        'favorite', 'coin', 'share', 'like', 'short_link',\n",
    "        'pub_location', 'pubdate', 'rcmd_reason'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, proxy_pool):\n",
    "        self.proxy_pool = proxy_pool\n",
    "        self.headers = headers.copy()\n",
    "        self.df_lst = []\n",
    "        self.complete_pages = []\n",
    "        self.error_page = []\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def fetch_data(self, page):\n",
    "        url = f\"https://api.bilibili.com/x/web-interface/popular/series/one?number={page}\"\n",
    "        \n",
    "        max_retries = 10\n",
    "        retry_count = 0\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            proxy = self.proxy_pool.get_proxy()\n",
    "            if not proxy:\n",
    "                print(f\"Page {page}: 没有可用代理\")\n",
    "                return False\n",
    "                \n",
    "            try:\n",
    "                proxies = self.proxy_pool.get_proxies_for_requests(proxy)\n",
    "                response = requests.get(url, headers=self.headers, proxies=proxies, timeout=10)\n",
    "                data = response.json()\n",
    "                \n",
    "                if data.get('code') == 0 and 'data' in data and 'list' in data.get('data', {}):\n",
    "                    data_list = data['data']['list']\n",
    "                    data_config = data['data']['config']\n",
    "                    \n",
    "                    date_name = data_config.get('name')\n",
    "                    \n",
    "                    page_data = []\n",
    "                    for item in data_list:\n",
    "                        tmp = []\n",
    "                        tmp.append(date_name)                                  \n",
    "                        tmp.append(item.get('aid', None))                      \n",
    "                        tmp.append(item.get('tid', None))                       \n",
    "                        tmp.append(item.get('tname', None))                     \n",
    "                        tmp.append(item.get('title', None))                    \n",
    "                        tmp.append(item.get('duration', None))                  \n",
    "                        owner = item.get('owner', {})\n",
    "                        tmp.append(owner.get('mid', None))                    \n",
    "                        tmp.append(owner.get('name', None))                    \n",
    "                        stat = item.get('stat', {})\n",
    "                        tmp.append(stat.get('view', None))                     \n",
    "                        tmp.append(stat.get('danmaku', None))                  \n",
    "                        tmp.append(stat.get('reply', None))                    \n",
    "                        tmp.append(stat.get('favorite', None))                \n",
    "                        tmp.append(stat.get('coin', None))                    \n",
    "                        tmp.append(stat.get('share', None))                   \n",
    "                        tmp.append(stat.get('like', None))                    \n",
    "                        tmp.append(item.get('short_link_v2', None))            \n",
    "                        tmp.append(item.get('pub_location', None))            \n",
    "                        tmp.append(item.get('pubdate', None))                 \n",
    "                        tmp.append(item.get('rcmd_reason', None))              \n",
    "\n",
    "                        page_data.append(tmp)\n",
    "                    \n",
    "                    with self.lock:\n",
    "                        self.df_lst.extend(page_data)\n",
    "                        self.complete_pages.append(page)\n",
    "                    \n",
    "                    print(f\"✅ 成功获取第 {page} 页数据，共 {len(page_data)} 条记录\")\n",
    "                    return True\n",
    "                \n",
    "                else:\n",
    "                    print(f\"Page {page}: 数据获取失败 - {data.get('message', 'Unknown error')}\")\n",
    "                    self.proxy_pool.remove_proxy(proxy)\n",
    "                    retry_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Page {page}: 请求异常 - {str(e)}\")\n",
    "                self.proxy_pool.remove_proxy(proxy)\n",
    "                retry_count += 1\n",
    "        \n",
    "        print(f\"Page {page}: 达到最大重试次数 {max_retries} 次，放弃\")\n",
    "        with self.lock:\n",
    "            self.error_page.append(page)\n",
    "        return False\n",
    "    \n",
    "    def run_concurrent_crawler(self, \n",
    "                               start_page, \n",
    "                               end_page, \n",
    "                               num_threads=5):\n",
    "        print(f\"\\n开始多线程爬取，页码范围: {start_page}-{end_page}，线程数: {num_threads}\\n\")\n",
    "        \n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            futures = {executor.submit(self.fetch_data, page): page for page in range(start_page, end_page+1)}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                page = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        success_count += 1\n",
    "                    else:\n",
    "                        fail_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Page {page}: 线程执行异常 - {str(e)}\")\n",
    "                    fail_count += 1\n",
    "        \n",
    "        print(f\"\\n爬取完成 - 成功: {success_count} 失败: {fail_count} 成功率: {success_count/(success_count+fail_count)*100:.1f}%\")\n",
    "    \n",
    "    def save_to_csv(self, filename=\"bilibili_popular.csv\"):\n",
    "        if not self.df_lst:\n",
    "            print(\"没有数据可保存\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            os.makedirs('./data', exist_ok=True)\n",
    "            df = pd.DataFrame(self.df_lst, columns=self.COLUMNS)\n",
    "            df = df.drop_duplicates(subset='aid', keep='first')\n",
    "            filepath = f\"./data/{filename}\"\n",
    "            df.to_csv(filepath, index=True, encoding='utf_8_sig')\n",
    "            \n",
    "            print(f\"数据已成功保存到 {filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"保存文件时出错: {str(e)}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfac3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_newest_page():\n",
    "    url = \"https://api.bilibili.com/x/web-interface/popular/series/list\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data.get('code') == 0 and data.get('data', {}).get('list', []):\n",
    "            newest_page = data['data']['list'][0].get('number', 1)\n",
    "            return newest_page\n",
    "    else:\n",
    "        print(f\"Error fetching newest page: {data.get('message', 'Unknown error')}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a14f8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始代理数量: 10\n",
      "\n",
      "开始多线程爬取，页码范围: 1-50，线程数: 10\n",
      "\n",
      "✅ 成功获取第 4 页数据，共 14 条记录\n",
      "✅ 成功获取第 6 页数据，共 12 条记录\n",
      "✅ 成功获取第 1 页数据，共 10 条记录\n",
      "✅ 成功获取第 2 页数据，共 15 条记录\n",
      "✅ 成功获取第 10 页数据，共 13 条记录\n",
      "✅ 成功获取第 9 页数据，共 14 条记录\n",
      "✅ 成功获取第 13 页数据，共 12 条记录\n",
      "✅ 成功获取第 7 页数据，共 12 条记录\n",
      "✅ 成功获取第 12 页数据，共 11 条记录\n",
      "✅ 成功获取第 14 页数据，共 13 条记录\n",
      "✅ 成功获取第 3 页数据，共 13 条记录\n",
      "✅ 成功获取第 15 页数据，共 11 条记录\n",
      "✅ 成功获取第 16 页数据，共 13 条记录\n",
      "✅ 成功获取第 11 页数据，共 13 条记录\n",
      "✅ 成功获取第 20 页数据，共 12 条记录\n",
      "✅ 成功获取第 17 页数据，共 13 条记录\n",
      "✅ 成功获取第 18 页数据，共 12 条记录\n",
      "✅ 成功获取第 22 页数据，共 14 条记录\n",
      "✅ 成功获取第 19 页数据，共 13 条记录\n",
      "✅ 成功获取第 23 页数据，共 17 条记录\n",
      "✅ 成功获取第 24 页数据，共 12 条记录\n",
      "✅ 成功获取第 27 页数据，共 22 条记录\n",
      "✅ 成功获取第 25 页数据，共 23 条记录\n",
      "✅ 成功获取第 30 页数据，共 24 条记录\n",
      "✅ 成功获取第 28 页数据，共 25 条记录\n",
      "✅ 成功获取第 31 页数据，共 22 条记录\n",
      "✅ 成功获取第 34 页数据，共 22 条记录\n",
      "✅ 成功获取第 32 页数据，共 16 条记录\n",
      "✅ 成功获取第 36 页数据，共 18 条记录\n",
      "✅ 成功获取第 5 页数据，共 13 条记录\n",
      "✅ 成功获取第 8 页数据，共 11 条记录\n",
      "✅ 成功获取第 38 页数据，共 16 条记录\n",
      "✅ 成功获取第 37 页数据，共 18 条记录\n",
      "✅ 成功获取第 41 页数据，共 22 条记录\n",
      "✅ 成功获取第 40 页数据，共 21 条记录\n",
      "✅ 成功获取第 39 页数据，共 23 条记录\n",
      "✅ 成功获取第 45 页数据，共 25 条记录\n",
      "✅ 成功获取第 42 页数据，共 23 条记录\n",
      "✅ 成功获取第 44 页数据，共 23 条记录\n",
      "✅ 成功获取第 47 页数据，共 25 条记录\n",
      "✅ 成功获取第 48 页数据，共 25 条记录\n",
      "✅ 成功获取第 49 页数据，共 25 条记录\n",
      "✅ 成功获取第 21 页数据，共 12 条记录\n",
      "移除了 4 个无效代理\n",
      "Page 26: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Read timed out. (read timeout=10)\n",
      "Page 29: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Read timed out. (read timeout=10)\n",
      "✅ 成功获取第 29 页数据，共 23 条记录\n",
      "✅ 成功获取第 33 页数据，共 22 条记录\n",
      "Page 43: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Read timed out. (read timeout=10)\n",
      "Page 46: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Read timed out. (read timeout=10)\n",
      "✅ 成功获取第 43 页数据，共 23 条记录\n",
      "✅ 成功获取第 46 页数据，共 23 条记录\n",
      "Page 35: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Read timed out. (read timeout=10)\n",
      "已补充 10 个新代理\n",
      "✅ 成功获取第 35 页数据，共 23 条记录\n",
      "✅ 成功获取第 50 页数据，共 27 条记录\n",
      "Page 26: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Read timed out. (read timeout=10)\n",
      "✅ 成功获取第 26 页数据，共 18 条记录\n",
      "\n",
      "爬取完成 - 成功: 50 失败: 0 成功率: 100.0%\n",
      "\n",
      "开始多线程爬取，页码范围: 51-100，线程数: 10\n",
      "\n",
      "Page 51: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Max retries exceeded with url: /x/web-interface/popular/series/one?number=51 (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 410 Gone')))\n",
      "Page 57: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Max retries exceeded with url: /x/web-interface/popular/series/one?number=57 (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 410 Gone')))\n",
      "✅ 成功获取第 59 页数据，共 28 条记录\n",
      "✅ 成功获取第 56 页数据，共 23 条记录\n",
      "✅ 成功获取第 58 页数据，共 25 条记录\n",
      "✅ 成功获取第 54 页数据，共 26 条记录\n",
      "✅ 成功获取第 52 页数据，共 25 条记录\n",
      "✅ 成功获取第 51 页数据，共 27 条记录\n",
      "✅ 成功获取第 63 页数据，共 27 条记录\n",
      "✅ 成功获取第 61 页数据，共 26 条记录\n",
      "✅ 成功获取第 55 页数据，共 28 条记录\n",
      "✅ 成功获取第 67 页数据，共 26 条记录\n",
      "✅ 成功获取第 68 页数据，共 24 条记录\n",
      "✅ 成功获取第 69 页数据，共 28 条记录\n",
      "✅ 成功获取第 60 页数据，共 26 条记录\n",
      "✅ 成功获取第 53 页数据，共 29 条记录\n",
      "✅ 成功获取第 64 页数据，共 25 条记录\n",
      "✅ 成功获取第 65 页数据，共 25 条记录\n",
      "✅ 成功获取第 70 页数据，共 27 条记录\n",
      "✅ 成功获取第 73 页数据，共 30 条记录\n",
      "✅ 成功获取第 74 页数据，共 26 条记录\n",
      "✅ 成功获取第 76 页数据，共 28 条记录\n",
      "✅ 成功获取第 72 页数据，共 26 条记录\n",
      "✅ 成功获取第 75 页数据，共 27 条记录\n",
      "✅ 成功获取第 78 页数据，共 28 条记录\n",
      "✅ 成功获取第 77 页数据，共 26 条记录\n",
      "✅ 成功获取第 79 页数据，共 24 条记录\n",
      "✅ 成功获取第 81 页数据，共 24 条记录\n",
      "✅ 成功获取第 82 页数据，共 28 条记录\n",
      "✅ 成功获取第 83 页数据，共 25 条记录\n",
      "✅ 成功获取第 85 页数据，共 28 条记录\n",
      "✅ 成功获取第 84 页数据，共 27 条记录\n",
      "✅ 成功获取第 71 页数据，共 26 条记录\n",
      "✅ 成功获取第 86 页数据，共 20 条记录\n",
      "✅ 成功获取第 88 页数据，共 27 条记录\n",
      "✅ 成功获取第 66 页数据，共 27 条记录\n",
      "✅ 成功获取第 91 页数据，共 28 条记录\n",
      "✅ 成功获取第 90 页数据，共 25 条记录\n",
      "✅ 成功获取第 93 页数据，共 30 条记录\n",
      "✅ 成功获取第 89 页数据，共 26 条记录\n",
      "✅ 成功获取第 87 页数据，共 24 条记录\n",
      "✅ 成功获取第 96 页数据，共 29 条记录\n",
      "✅ 成功获取第 95 页数据，共 29 条记录\n",
      "✅ 成功获取第 97 页数据，共 31 条记录\n",
      "✅ 成功获取第 99 页数据，共 37 条记录\n",
      "✅ 成功获取第 100 页数据，共 30 条记录\n",
      "✅ 成功获取第 92 页数据，共 29 条记录\n",
      "✅ 成功获取第 94 页数据，共 30 条记录\n",
      "✅ 成功获取第 98 页数据，共 31 条记录\n",
      "Page 62: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Read timed out. (read timeout=10)\n",
      "✅ 成功获取第 62 页数据，共 25 条记录\n",
      "✅ 成功获取第 80 页数据，共 28 条记录\n",
      "Page 57: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Read timed out. (read timeout=10)\n",
      "✅ 成功获取第 57 页数据，共 21 条记录\n",
      "\n",
      "爬取完成 - 成功: 50 失败: 0 成功率: 100.0%\n",
      "\n",
      "开始多线程爬取，页码范围: 101-150，线程数: 10\n",
      "\n",
      "✅ 成功获取第 103 页数据，共 33 条记录\n",
      "✅ 成功获取第 102 页数据，共 32 条记录\n",
      "✅ 成功获取第 104 页数据，共 34 条记录\n",
      "✅ 成功获取第 106 页数据，共 39 条记录\n",
      "✅ 成功获取第 109 页数据，共 39 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)\n",
      "✅ 成功获取第 108 页数据，共 40 条记录\n",
      "✅ 成功获取第 107 页数据，共 33 条记录\n",
      "✅ 成功获取第 112 页数据，共 34 条记录\n",
      "✅ 成功获取第 115 页数据，共 40 条记录\n",
      "✅ 成功获取第 113 页数据，共 38 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)\n",
      "✅ 成功获取第 118 页数据，共 38 条记录\n",
      "✅ 成功获取第 111 页数据，共 38 条记录\n",
      "✅ 成功获取第 117 页数据，共 34 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)\n",
      "✅ 成功获取第 121 页数据，共 35 条记录\n",
      "✅ 成功获取第 122 页数据，共 37 条记录\n",
      "✅ 成功获取第 101 页数据，共 34 条记录\n",
      "✅ 成功获取第 124 页数据，共 35 条记录\n",
      "✅ 成功获取第 119 页数据，共 39 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)\n",
      "✅ 成功获取第 125 页数据，共 39 条记录\n",
      "✅ 成功获取第 128 页数据，共 43 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)✅ 成功获取第 129 页数据，共 41 条记录\n",
      "\n",
      "✅ 成功获取第 130 页数据，共 36 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)\n",
      "✅ 成功获取第 127 页数据，共 36 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)\n",
      "✅ 成功获取第 123 页数据，共 33 条记录\n",
      "✅ 成功获取第 133 页数据，共 38 条记录\n",
      "已补充 10 个新代理\n",
      "✅ 成功获取第 116 页数据，共 39 条记录\n",
      "✅ 成功获取第 134 页数据，共 41 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)\n",
      "✅ 成功获取第 110 页数据，共 38 条记录\n",
      "✅ 成功获取第 136 页数据，共 42 条记录\n",
      "✅ 成功获取第 137 页数据，共 41 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)\n",
      "✅ 成功获取第 138 页数据，共 43 条记录\n",
      "✅ 成功获取第 139 页数据，共 42 条记录\n",
      "✅ 成功获取第 140 页数据，共 35 条记录\n",
      "✅ 成功获取第 142 页数据，共 41 条记录\n",
      "✅ 成功获取第 143 页数据，共 42 条记录\n",
      "✅ 成功获取第 131 页数据，共 38 条记录\n",
      "✅ 成功获取第 145 页数据，共 41 条记录\n",
      "✅ 成功获取第 141 页数据，共 41 条记录\n",
      "Page 105: 请求异常 - Expecting value: line 1 column 1 (char 0)\n",
      "Page 105: 达到最大重试次数 10 次，放弃\n",
      "✅ 成功获取第 146 页数据，共 44 条记录\n",
      "✅ 成功获取第 147 页数据，共 39 条记录\n",
      "✅ 成功获取第 150 页数据，共 47 条记录\n",
      "✅ 成功获取第 148 页数据，共 43 条记录\n",
      "✅ 成功获取第 132 页数据，共 36 条记录\n",
      "✅ 成功获取第 144 页数据，共 43 条记录\n",
      "✅ 成功获取第 149 页数据，共 47 条记录\n",
      "✅ 成功获取第 114 页数据，共 38 条记录\n",
      "✅ 成功获取第 135 页数据，共 37 条记录\n",
      "✅ 成功获取第 120 页数据，共 40 条记录\n",
      "Page 126: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Read timed out.\n",
      "Page 126: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Max retries exceeded with url: /x/web-interface/popular/series/one?number=126 (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 407 Proxy Authentication Required')))\n",
      "✅ 成功获取第 126 页数据，共 42 条记录\n",
      "\n",
      "爬取完成 - 成功: 49 失败: 1 成功率: 98.0%\n",
      "\n",
      "开始多线程爬取，页码范围: 151-200，线程数: 10\n",
      "\n",
      "✅ 成功获取第 155 页数据，共 45 条记录\n",
      "✅ 成功获取第 153 页数据，共 46 条记录\n",
      "✅ 成功获取第 156 页数据，共 47 条记录\n",
      "✅ 成功获取第 157 页数据，共 41 条记录\n",
      "✅ 成功获取第 154 页数据，共 48 条记录\n",
      "✅ 成功获取第 158 页数据，共 48 条记录\n",
      "✅ 成功获取第 151 页数据，共 50 条记录\n",
      "✅ 成功获取第 152 页数据，共 48 条记录\n",
      "✅ 成功获取第 160 页数据，共 44 条记录\n",
      "✅ 成功获取第 162 页数据，共 33 条记录\n",
      "✅ 成功获取第 163 页数据，共 34 条记录\n",
      "✅ 成功获取第 164 页数据，共 41 条记录\n",
      "✅ 成功获取第 165 页数据，共 43 条记录\n",
      "✅ 成功获取第 166 页数据，共 46 条记录\n",
      "✅ 成功获取第 167 页数据，共 33 条记录\n",
      "✅ 成功获取第 161 页数据，共 40 条记录\n",
      "Page 176: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Max retries exceeded with url: /x/web-interface/popular/series/one?number=176 (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 407 Proxy Authentication Required')))\n",
      "✅ 成功获取第 168 页数据，共 37 条记录\n",
      "✅ 成功获取第 169 页数据，共 42 条记录\n",
      "✅ 成功获取第 170 页数据，共 48 条记录\n",
      "Page 178: 请求异常 - HTTPSConnectionPool(host='api.bilibili.com', port=443): Max retries exceeded with url: /x/web-interface/popular/series/one?number=178 (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 407 Proxy Authentication Required')))\n",
      "✅ 成功获取第 172 页数据，共 42 条记录\n",
      "✅ 成功获取第 174 页数据，共 48 条记录\n",
      "✅ 成功获取第 171 页数据，共 45 条记录\n",
      "✅ 成功获取第 173 页数据，共 45 条记录\n",
      "✅ 成功获取第 175 页数据，共 43 条记录\n",
      "✅ 成功获取第 176 页数据，共 45 条记录\n",
      "✅ 成功获取第 179 页数据，共 37 条记录\n",
      "✅ 成功获取第 182 页数据，共 47 条记录\n",
      "✅ 成功获取第 183 页数据，共 32 条记录\n",
      "✅ 成功获取第 181 页数据，共 42 条记录\n",
      "✅ 成功获取第 184 页数据，共 43 条记录\n",
      "✅ 成功获取第 177 页数据，共 42 条记录\n",
      "✅ 成功获取第 185 页数据，共 47 条记录\n",
      "✅ 成功获取第 186 页数据，共 40 条记录\n",
      "✅ 成功获取第 178 页数据，共 48 条记录\n",
      "✅ 成功获取第 191 页数据，共 46 条记录\n",
      "✅ 成功获取第 189 页数据，共 48 条记录\n",
      "✅ 成功获取第 192 页数据，共 47 条记录\n",
      "✅ 成功获取第 188 页数据，共 49 条记录\n",
      "✅ 成功获取第 194 页数据，共 46 条记录\n",
      "✅ 成功获取第 193 页数据，共 44 条记录\n",
      "✅ 成功获取第 190 页数据，共 49 条记录\n",
      "✅ 成功获取第 187 页数据，共 49 条记录\n",
      "✅ 成功获取第 195 页数据，共 46 条记录\n",
      "✅ 成功获取第 198 页数据，共 49 条记录\n",
      "✅ 成功获取第 200 页数据，共 49 条记录\n",
      "✅ 成功获取第 196 页数据，共 49 条记录\n",
      "✅ 成功获取第 199 页数据，共 50 条记录\n",
      "✅ 成功获取第 197 页数据，共 48 条记录\n",
      "✅ 成功获取第 159 页数据，共 48 条记录\n",
      "✅ 成功获取第 180 页数据，共 47 条记录\n",
      "\n",
      "爬取完成 - 成功: 50 失败: 0 成功率: 100.0%\n",
      "\n",
      "开始多线程爬取，页码范围: 201-250，线程数: 10\n",
      "\n",
      "✅ 成功获取第 207 页数据，共 47 条记录\n",
      "✅ 成功获取第 210 页数据，共 37 条记录\n",
      "✅ 成功获取第 203 页数据，共 41 条记录\n",
      "✅ 成功获取第 202 页数据，共 44 条记录\n",
      "✅ 成功获取第 204 页数据，共 44 条记录\n",
      "✅ 成功获取第 206 页数据，共 46 条记录\n",
      "✅ 成功获取第 208 页数据，共 39 条记录\n",
      "✅ 成功获取第 201 页数据，共 43 条记录\n",
      "✅ 成功获取第 211 页数据，共 48 条记录\n",
      "✅ 成功获取第 217 页数据，共 43 条记录\n",
      "✅ 成功获取第 214 页数据，共 42 条记录\n",
      "✅ 成功获取第 215 页数据，共 49 条记录\n",
      "✅ 成功获取第 212 页数据，共 43 条记录\n",
      "✅ 成功获取第 218 页数据，共 45 条记录\n",
      "✅ 成功获取第 216 页数据，共 39 条记录\n",
      "✅ 成功获取第 220 页数据，共 38 条记录\n",
      "✅ 成功获取第 219 页数据，共 41 条记录\n",
      "✅ 成功获取第 223 页数据，共 41 条记录\n",
      "✅ 成功获取第 222 页数据，共 39 条记录\n",
      "✅ 成功获取第 221 页数据，共 47 条记录\n",
      "✅ 成功获取第 205 页数据，共 44 条记录\n",
      "✅ 成功获取第 224 页数据，共 39 条记录\n",
      "✅ 成功获取第 225 页数据，共 46 条记录\n",
      "✅ 成功获取第 226 页数据，共 43 条记录\n",
      "✅ 成功获取第 213 页数据，共 46 条记录\n",
      "✅ 成功获取第 231 页数据，共 48 条记录\n",
      "✅ 成功获取第 229 页数据，共 43 条记录\n",
      "✅ 成功获取第 227 页数据，共 43 条记录\n",
      "✅ 成功获取第 228 页数据，共 47 条记录\n",
      "✅ 成功获取第 232 页数据，共 41 条记录\n",
      "✅ 成功获取第 209 页数据，共 48 条记录\n",
      "✅ 成功获取第 230 页数据，共 36 条记录\n",
      "✅ 成功获取第 233 页数据，共 40 条记录\n",
      "✅ 成功获取第 235 页数据，共 34 条记录\n",
      "✅ 成功获取第 237 页数据，共 38 条记录\n",
      "✅ 成功获取第 239 页数据，共 43 条记录\n",
      "✅ 成功获取第 240 页数据，共 42 条记录\n",
      "✅ 成功获取第 242 页数据，共 38 条记录\n",
      "✅ 成功获取第 236 页数据，共 47 条记录\n",
      "✅ 成功获取第 241 页数据，共 37 条记录\n",
      "✅ 成功获取第 243 页数据，共 40 条记录\n",
      "✅ 成功获取第 234 页数据，共 37 条记录\n",
      "✅ 成功获取第 245 页数据，共 39 条记录\n",
      "✅ 成功获取第 244 页数据，共 37 条记录\n",
      "✅ 成功获取第 238 页数据，共 35 条记录\n",
      "✅ 成功获取第 246 页数据，共 44 条记录\n",
      "✅ 成功获取第 250 页数据，共 38 条记录\n",
      "✅ 成功获取第 249 页数据，共 44 条记录\n",
      "✅ 成功获取第 247 页数据，共 42 条记录\n",
      "✅ 成功获取第 248 页数据，共 39 条记录\n",
      "\n",
      "爬取完成 - 成功: 50 失败: 0 成功率: 100.0%\n",
      "\n",
      "开始多线程爬取，页码范围: 251-300，线程数: 10\n",
      "\n",
      "✅ 成功获取第 257 页数据，共 36 条记录\n",
      "✅ 成功获取第 252 页数据，共 40 条记录\n",
      "✅ 成功获取第 255 页数据，共 43 条记录\n",
      "✅ 成功获取第 254 页数据，共 46 条记录\n",
      "✅ 成功获取第 260 页数据，共 46 条记录\n",
      "✅ 成功获取第 259 页数据，共 41 条记录\n",
      "✅ 成功获取第 251 页数据，共 43 条记录\n",
      "✅ 成功获取第 256 页数据，共 44 条记录\n",
      "✅ 成功获取第 258 页数据，共 45 条记录\n",
      "✅ 成功获取第 261 页数据，共 43 条记录\n",
      "✅ 成功获取第 268 页数据，共 42 条记录\n",
      "✅ 成功获取第 264 页数据，共 44 条记录\n",
      "✅ 成功获取第 263 页数据，共 39 条记录\n",
      "✅ 成功获取第 265 页数据，共 39 条记录\n",
      "✅ 成功获取第 262 页数据，共 45 条记录\n",
      "✅ 成功获取第 267 页数据，共 40 条记录\n",
      "✅ 成功获取第 253 页数据，共 43 条记录\n",
      "✅ 成功获取第 266 页数据，共 45 条记录\n",
      "✅ 成功获取第 275 页数据，共 45 条记录\n",
      "✅ 成功获取第 270 页数据，共 49 条记录\n",
      "✅ 成功获取第 274 页数据，共 45 条记录\n",
      "✅ 成功获取第 273 页数据，共 44 条记录\n",
      "✅ 成功获取第 272 页数据，共 40 条记录\n",
      "✅ 成功获取第 277 页数据，共 40 条记录\n",
      "✅ 成功获取第 269 页数据，共 41 条记录\n",
      "✅ 成功获取第 278 页数据，共 42 条记录\n",
      "✅ 成功获取第 281 页数据，共 44 条记录\n",
      "✅ 成功获取第 271 页数据，共 44 条记录\n",
      "✅ 成功获取第 280 页数据，共 45 条记录\n",
      "✅ 成功获取第 276 页数据，共 36 条记录\n",
      "✅ 成功获取第 279 页数据，共 42 条记录\n",
      "✅ 成功获取第 282 页数据，共 41 条记录\n",
      "✅ 成功获取第 284 页数据，共 40 条记录\n",
      "✅ 成功获取第 283 页数据，共 36 条记录\n",
      "✅ 成功获取第 286 页数据，共 44 条记录\n",
      "✅ 成功获取第 287 页数据，共 44 条记录\n",
      "✅ 成功获取第 290 页数据，共 38 条记录\n",
      "✅ 成功获取第 289 页数据，共 37 条记录\n",
      "✅ 成功获取第 292 页数据，共 38 条记录\n",
      "✅ 成功获取第 288 页数据，共 34 条记录\n",
      "✅ 成功获取第 293 页数据，共 50 条记录\n",
      "✅ 成功获取第 285 页数据，共 50 条记录\n",
      "✅ 成功获取第 291 页数据，共 40 条记录\n",
      "✅ 成功获取第 296 页数据，共 49 条记录\n",
      "✅ 成功获取第 298 页数据，共 44 条记录\n",
      "✅ 成功获取第 297 页数据，共 50 条记录\n",
      "✅ 成功获取第 294 页数据，共 49 条记录\n",
      "✅ 成功获取第 295 页数据，共 43 条记录\n",
      "✅ 成功获取第 300 页数据，共 45 条记录\n",
      "✅ 成功获取第 299 页数据，共 50 条记录\n",
      "\n",
      "爬取完成 - 成功: 50 失败: 0 成功率: 100.0%\n",
      "\n",
      "开始多线程爬取，页码范围: 301-324，线程数: 10\n",
      "\n",
      "✅ 成功获取第 310 页数据，共 43 条记录\n",
      "✅ 成功获取第 307 页数据，共 49 条记录\n",
      "✅ 成功获取第 302 页数据，共 47 条记录\n",
      "✅ 成功获取第 305 页数据，共 45 条记录\n",
      "✅ 成功获取第 308 页数据，共 49 条记录\n",
      "✅ 成功获取第 309 页数据，共 46 条记录\n",
      "✅ 成功获取第 301 页数据，共 49 条记录\n",
      "✅ 成功获取第 304 页数据，共 49 条记录\n",
      "✅ 成功获取第 311 页数据，共 48 条记录\n",
      "✅ 成功获取第 314 页数据，共 48 条记录\n",
      "✅ 成功获取第 316 页数据，共 44 条记录\n",
      "✅ 成功获取第 315 页数据，共 46 条记录\n",
      "✅ 成功获取第 313 页数据，共 45 条记录\n",
      "✅ 成功获取第 312 页数据，共 42 条记录\n",
      "✅ 成功获取第 317 页数据，共 50 条记录\n",
      "✅ 成功获取第 303 页数据，共 46 条记录\n",
      "✅ 成功获取第 319 页数据，共 48 条记录\n",
      "✅ 成功获取第 320 页数据，共 42 条记录\n",
      "✅ 成功获取第 322 页数据，共 40 条记录\n",
      "✅ 成功获取第 323 页数据，共 41 条记录\n",
      "✅ 成功获取第 318 页数据，共 49 条记录\n",
      "✅ 成功获取第 324 页数据，共 41 条记录\n",
      "✅ 成功获取第 321 页数据，共 48 条记录\n",
      "✅ 成功获取第 306 页数据，共 42 条记录\n",
      "\n",
      "爬取完成 - 成功: 24 失败: 0 成功率: 100.0%\n",
      "\n",
      "程序退出\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "移除了 4 个无效代理\n"
     ]
    }
   ],
   "source": [
    "proxy_pool = ProxyPool(auth_key=os.getenv('AUTH_KEY'), password=os.getenv('AUTH_PASSWORD'))\n",
    "\n",
    "initial_proxies = proxy_pool.fetch_proxies(10)\n",
    "proxy_pool.add_proxies(initial_proxies)\n",
    "print(f\"初始代理数量: {len(proxy_pool.proxy_pool)}\")\n",
    "\n",
    "maintain_thread = threading.Thread(target=proxy_pool.auto_maintain, daemon=True)\n",
    "maintain_thread.start()\n",
    "\n",
    "crawler = BilibiliCrawler(proxy_pool)\n",
    "\n",
    "mmax_page, batch = get_newest_page(), 50\n",
    "for i in range(mmax_page // batch + 1):\n",
    "    crawler.run_concurrent_crawler(start_page=i*batch+1, \n",
    "                                    end_page=min((i+1)*batch, mmax_page), \n",
    "                                    num_threads=10)\n",
    "    time.sleep(5)   \n",
    "\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if len(crawler.complete_pages) == mmax_page:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n程序退出\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a30857",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.bilibili.com/x/web-interface/view/detail?aid=\"\n",
    "df = pd.read_csv(\"./data/bilibili_popular.csv\", encoding='utf_8_sig')\n",
    "aids = df['aid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f171d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tag(aid):\n",
    "    url = base_url + str(aid)\n",
    "    proxy = proxy_pool.get_proxy()\n",
    "    proxies = proxy_pool.get_proxies_for_requests(proxy)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, proxies=proxies, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data['code'] == 0:\n",
    "                tags = [tag['tag_name'] for tag in data.get('data', {}).get('Tags', [])]\n",
    "            else:\n",
    "                print(f\"获取aid {aid}的标签失败: {data.get('message', 'Unknown error')}, {'code': data.get('code')}\")\n",
    "                tags = None\n",
    "            return (aid, tags)\n",
    "        else:\n",
    "            print(f\"获取aid {aid}失败，状态码: {response.status_code}\")\n",
    "            proxy_pool.remove_proxy(proxy)  \n",
    "    except Exception as e:\n",
    "        print(f\"获取aid {aid}时出错: {str(e)}\")\n",
    "        proxy_pool.remove_proxy(proxy)  \n",
    "    return (aid, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3405d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11641 entries, 0 to 11640\n",
      "Data columns (total 19 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date_name     11641 non-null  object\n",
      " 1   aid           11641 non-null  int64 \n",
      " 2   tid           11641 non-null  int64 \n",
      " 3   tname         11641 non-null  object\n",
      " 4   title         11641 non-null  object\n",
      " 5   duration      11641 non-null  int64 \n",
      " 6   owner_mid     11641 non-null  int64 \n",
      " 7   owner_name    11641 non-null  object\n",
      " 8   view          11641 non-null  int64 \n",
      " 9   danmaku       11641 non-null  int64 \n",
      " 10  reply         11641 non-null  int64 \n",
      " 11  favorite      11641 non-null  int64 \n",
      " 12  coin          11641 non-null  int64 \n",
      " 13  share         11641 non-null  int64 \n",
      " 14  like          11641 non-null  int64 \n",
      " 15  short_link    11641 non-null  object\n",
      " 16  pub_location  7659 non-null   object\n",
      " 17  pubdate       11641 non-null  int64 \n",
      " 18  rcmd_reason   11597 non-null  object\n",
      "dtypes: int64(12), object(7)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/bilibili_popular.csv\", encoding='utf_8_sig')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9847153",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582733e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was an error managing chromedriver (error sending request for url (https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json)); using driver found in the cache\n"
     ]
    }
   ],
   "source": [
    "# 获取分区 将tag归为大的分区\n",
    "# 需开启代理 否则国内访问github会被限制\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "url = \"https://github.com/SocialSisterYi/bilibili-API-collect/blob/master/docs/video/video_zone.md\"\n",
    "driver.get(url)\n",
    "time.sleep(2)  \n",
    "\n",
    "html_content = driver.page_source\n",
    "\n",
    "tree = html.fromstring(html_content)\n",
    "class_dict = {}\n",
    "\n",
    "for i in range(2, 24):\n",
    "    xpath = f'//*[@id=\"repo-content-pjax-container\"]/react-app/div/div/div[1]/div/div/div[2]/div/div/div[3]/div[2]/div/div[3]/section/div/article'\n",
    "    main_xpath = f'{xpath}/div[{i}]/h2/text()'\n",
    "    class_fa = tree.xpath(main_xpath)[0].strip()\n",
    "    sub_xpath = f'{xpath}/markdown-accessiblity-table[{i-1}]/table/tbody/tr'\n",
    "    sub_elements = tree.xpath(sub_xpath)\n",
    "    for sub_element in sub_elements:\n",
    "        try:\n",
    "            class_son_id = sub_element.xpath('td[3]/text()')[0].strip()\n",
    "        except IndexError:\n",
    "            class_son_id = sub_element.xpath('td[3]/del/text()')[0].strip()\n",
    "        if class_dict.get(class_fa) is None:\n",
    "            class_dict[class_fa] = []\n",
    "        class_dict[class_fa].append(class_son_id)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "261e4e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重复的ID: 176 在 生活 中\n",
      "重复的ID: 76 在 美食 中\n",
      "重复的ID: 75 在 动物圈 中\n",
      "重复的ID: 164 在 时尚 中\n"
     ]
    }
   ],
   "source": [
    "# 处理重定向的id 即换过分区的tag\n",
    "ids = []\n",
    "for class_fa, class_son_ids in class_dict.items():\n",
    "    for class_son_id in class_son_ids:\n",
    "        if class_son_id not in ids:\n",
    "            ids.append(class_son_id)\n",
    "        else:\n",
    "            print(f\"重复的ID: {class_son_id} 在 {class_fa} 中\")\n",
    "to_remove = [176, 76, 75, 164]\n",
    "for i in to_remove:\n",
    "    if i in class_dict.get('生活', []):\n",
    "        class_dict['生活'].remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d452b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有tid都已成功映射到分区\n"
     ]
    }
   ],
   "source": [
    "tid_to_fa = {}\n",
    "for class_fa, tid_list in class_dict.items():\n",
    "    for tid in tid_list:\n",
    "        tid_to_fa[int(tid)] = class_fa\n",
    "df['t_fa'] = df['tid'].map(tid_to_fa)\n",
    "df.head(10)\n",
    "\n",
    "unmapped_count = df['t_fa'].isna().sum()\n",
    "if unmapped_count > 0:\n",
    "    print(f\"警告：有 {unmapped_count} 行的tid无法映射到父分类\")\n",
    "    print(f\"未映射的tid值: {df[df['t_fa'].isna()]['tid'].unique()}\")\n",
    "else:\n",
    "    print(\"所有tid都已成功映射到分区\")\n",
    "    \n",
    "    df['t_fa'] = df['t_fa'].fillna(\"未分类\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847e7ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成，已保存到 ./data/bilibili_popular.csv\n"
     ]
    }
   ],
   "source": [
    "# 将时间戳转化为日期格式\n",
    "df['pudate'] = df['pubdate'].apply(lambda x: datetime.fromtimestamp(x, tz=pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d'))\n",
    "\n",
    "df.to_csv(\"./data/bilibili_popular.csv\", index=False, encoding='utf_8_sig')\n",
    "print(\"数据处理完成，已保存到 ./data/bilibili_popular.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
